{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;text-align:left;font-family:Tahoma\">\n",
    "    you can download all necessary data from this link\n",
    "    http://nlp.stanford.edu/data/glove.6B.zip\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir =\"C:/Users/ziaeeamir/Desktop/NLP/\" #data set adress should be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector comparison\n",
    "\n",
    "Vector comparisons form the heart of our analyses in this context.\n",
    "\n",
    "    For the most part, we are interested in measuring the distance between vectors. The guiding idea is that semantically related words should be close together in the vector spaces we build, and semantically unrelated words should be far apart.\n",
    "\n",
    "    The scipy.spatial.distance module has a lot of vector comparison methods, so you might check them out if you want to go beyond the functions defined and explored here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Euclidean distance\n",
    "The most basic and intuitive distance measure between vectors is __euclidean distance__. The euclidean distance between two vectors $u$ and $v$ of dimension $n$ is\n",
    "\n",
    "$$\\textbf{euclidean}(u, v) = \n",
    "\\sqrt{\\sum_{i=1}^{n}|u_{i} - v_{i}|^{2}}$$\n",
    "\n",
    "just uses the corresponding [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html) method to define it.\n",
    "\n",
    "Notice: befor the use of the euclidean distance you should perform Length normalization:\n",
    "<img src=\"img/5.JPG\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Cosine similarity\n",
    "\n",
    "To measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
    "\n",
    "where $u.v$ is the dot product (or inner product) of two vectors, $||u||_2$ is the norm (or length) of the vector $u$, and $\\theta$ is the angle between $u$ and $v$. This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. \n",
    "\n",
    "<img src=\"img/2.JPG\" style=\"width:800px;height:250px;\">\n",
    "<caption><center> **Figure 1**: The cosine of the angle between two vectors is a measure of how similar they are</center></caption>\n",
    "\n",
    "\n",
    "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Other Matching based methods\n",
    "\n",
    "another popular family of these comparison methods,called matching-based methods.the matching coefficient is the one that I've given at the top there.\n",
    "you are just summing up all of the smaller of the two values,doing a point-wise comparison across the vectors that you wanna compare.Jaccard, Dice,Overlap are all defined in terms of that matching thing.\n",
    "\n",
    "\n",
    "\n",
    "Matching-based methods are also common in the literature. The basic matching measure effectively creates a vector consisting of all of the smaller of the two values at each coordinate, and then sums them:\n",
    "\n",
    "$$\\textbf{matching}(u, v) = \\sum_{i=1}^{n} \\min(u_{i}, v_{i})$$\n",
    "\n",
    "This is implemented in `vsm` as `matching`.\n",
    "\n",
    "One approach to normalizing the matching values is the [__Jaccard coefficient__](https://en.wikipedia.org/wiki/Jaccard_index). The numerator is the matching coefficient. The denominator — the normalizer — is intuitively like the set union: for binary vectors, it gives the cardinality of the union of the two being compared:\n",
    "\n",
    "$$\\textbf{jaccard}(u, v) = \n",
    "1 - \\frac{\\textbf{matching}(u, v)}{\\sum_{i=1}^{n} \\max(u_{i}, v_{i})}$$\n",
    " \n",
    "\n",
    "<img src=\"img/4.JPG\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(u, v): #   u,v--->(100,)\n",
    "    return np.squeeze(cosine_similarity(u.reshape(1, -1), v.reshape(1, -1)))  #(u.reshape(1, -1)-->1, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.86566603\n",
      "cosine_similarity(ball, crocodile) =  0.15206575\n",
      "cosine_similarity(france - paris, iran - tehran) =  0.6854124\n"
     ]
    }
   ],
   "source": [
    "father = embeddings_index[\"father\"]\n",
    "mother = embeddings_index[\"mother\"]\n",
    "ball = embeddings_index[\"ball\"]\n",
    "crocodile = embeddings_index[\"crocodile\"]\n",
    "france = embeddings_index[\"france\"]\n",
    "iran = embeddings_index[\"iran\"]\n",
    "paris = embeddings_index[\"paris\"]\n",
    "tehran = embeddings_index[\"tehran\"]\n",
    "print(\"cosine_similarity(father, mother) = \", similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, iran - tehran) = \",similarity(france - paris, iran - tehran))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Word analogy task\n",
    "\n",
    "In the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. In detail, we are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n",
    "\n",
    "**Exercise**: Complete the code below to be able to perform word analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64706 , -0.068067,  0.15468 , -0.17408 , -0.29134 ,  0.76999 ,\n",
       "       -0.3192  , -0.25663 , -0.25082 , -0.036737, -0.25509 ,  0.29636 ,\n",
       "        0.5776  ,  0.49641 ,  0.19167 , -0.83888 ,  0.58482 , -0.38717 ,\n",
       "       -0.71591 ,  0.9519  , -0.37966 , -0.1131  ,  0.47154 ,  0.20921 ,\n",
       "        0.38197 ,  0.067582, -0.92879 , -1.1237  ,  0.84831 ,  0.68744 ,\n",
       "       -0.15472 ,  0.92714 ,  0.53371 , -0.037392, -0.856   ,  0.19056 ,\n",
       "       -0.014594,  0.15186 ,  0.53514 , -0.20306 , -0.35164 ,  0.33152 ,\n",
       "        1.1306  , -0.72787 , -0.19724 ,  0.031659, -0.24041 , -0.057617,\n",
       "        0.60473 , -0.49233 , -0.24405 , -0.3184  ,  0.96156 ,  1.0895  ,\n",
       "        0.21534 , -2.0542  , -1.0615  ,  0.052439,  0.57958 ,  0.2748  ,\n",
       "        0.91587 ,  0.85195 ,  0.36113 , -0.31901 ,  0.7784  , -0.36865 ,\n",
       "        0.64387 ,  0.33104 , -0.27181 ,  0.58524 , -0.15143 ,  0.11121 ,\n",
       "        0.2126  , -0.60345 ,  0.16148 ,  0.32952 , -0.1354  , -0.30629 ,\n",
       "       -0.89143 ,  0.091912,  0.49753 ,  0.55932 ,  0.19329 ,  0.044859,\n",
       "       -1.0416  , -0.41566 , -0.54174 , -0.7244  , -0.57492 , -1.1188  ,\n",
       "        0.087097, -0.2992  ,  0.87227 ,  0.86996 , -0.89641 , -0.28259 ,\n",
       "       -0.47295 , -0.74062 , -0.39    , -0.78099 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"father\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, embeddings_index):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings v_a, v_b and v_c \n",
    "    e_a, e_b, e_c = embeddings_index[word_a], embeddings_index[word_b], embeddings_index[word_c]\n",
    "    \n",
    "    words = embeddings_index.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = similarity(e_b - e_a, embeddings_index[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your code, this may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iranian'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_analogy('china', 'chinese', 'iran', embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tehran'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_analogy('india', 'delhi', 'iran', embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'girl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_analogy('man', 'woman', 'boy', embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigger'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_analogy('small', 'smaller', 'big', embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'henning'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_analogy('Dog', 'Wolf', 'Cat', embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<a href=\"https://github.com/A2Amir\">Amir Github</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
